{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation to compare out of sample performance of strategies\n",
    "\n",
    "First, we generate 10 series of random Gaussian returns (520 observations, equivalent to 2 years\n",
    "of daily history), with 0 mean and an arbitrary standard deviation of 10%. Real prices exhibit\n",
    "frequent jumps (Merton, 1976) and returns are not cross-sectionally independent, so we must add\n",
    "random shocks and a random correlation structure to our generated data. Second, we compute\n",
    "HRP, CLA and IVP portfolios by looking back at 260 observations (a year of daily history).\n",
    "These portfolios are re-estimated and rebalanced every 22 observations (equivalent to a monthly\n",
    "frequency). Third, we compute the out-of-sample returns associated with those three portfolios.\n",
    "This procedure is repeated 10,000 times.\n",
    "\n",
    "In intuitive terms, we can understand the above empirical results as follows: Shocks affecting a specific\n",
    "investment penalize CLA’s concentration. Shocks involving several correlated investments\n",
    "penalize IVP’s ignorance of the correlation structure. HRP provides better protection against\n",
    "both, common and idiosyncratic shocks, by finding a compromise between diversification across\n",
    "all investments and diversification across clusters of investments at multiple hierarchical levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing ipynb file : HRP_Original.ipynb\n",
      "Numiters 10.0\n",
      "Numiters 10.0\n",
      "            MEAN  ST. DEV.    Sharpe  VARIANCE  PERFORMANCE\n",
      "QPsol   0.289796  0.332794  0.870798  0.110752     2.650919\n",
      "getCLA  0.289795  0.332794  0.870794  0.110752     2.650928\n",
      "getHRP  0.173662  0.174170  0.997084  0.030335     0.000000\n",
      "getIVP  0.206845  0.230038  0.899180  0.052917     0.744412\n"
     ]
    }
   ],
   "source": [
    "import scipy.cluster.hierarchy as sch,random,numpy as np,pandas as pd,CLA\n",
    "import matplotlib.pyplot as mpl\n",
    "from HRP_Original import correlDist,getIVP,getQuasiDiag,getRecBipart,QPsol\n",
    "import xlsxwriter\n",
    "#------------------------------------------------------------------------------\n",
    "def generateData(nObs,sLength,size0,size1,mu0,sigma0,sigma1F):\n",
    "    # Time series of correlated variables\n",
    "    #1) generate random uncorrelated data\n",
    "    x=np.random.normal(mu0,sigma0,size=(nObs,size0)) # each row is a variable\n",
    "    #2) create correlation between the variables\n",
    "    cols=[random.randint(0,size0-1) for i in xrange(size1)]\n",
    "    #print cols\n",
    "    cols2 = [random.randint(0,size0-1) for i in xrange(size1)]\n",
    "    cols3 = [random.randint(0,size0-1) for i in xrange(size1)]\n",
    "    cols4 = [random.randint(0,size0-1) for i in xrange(size1)]\n",
    "    #x = x + x[:,cols3]/4 + x[:,cols]/2+np.random.normal(0,.1,size=(nObs,len(cols)))\n",
    "    #y=x[:,cols4]/8+x[:,cols3]/4+x[:,cols2]/2+x[:,cols]+np.random.normal(0,sigma0*sigma1F,size=(nObs,len(cols)))\n",
    "\n",
    "    y=x[:,cols]+np.random.normal(0,sigma0*sigma1F,size=(nObs,len(cols)))\n",
    "    x=np.append(x,y,axis=1)\n",
    "    #3) add common random shock\n",
    "    point=np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[np.ix_(point,[cols[0],size0])]=np.array([[-.5,-.5],[2,2]])\n",
    "    #print np.ix_(point,[cols[0],size0])\n",
    "    #4) add specific random shock\n",
    "    point=np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[point,cols[-1]]=np.array([-.5,2])\n",
    "    #print point,cols[-1]\n",
    "    x=pd.DataFrame(x,columns=range(1,x.shape[1]+1))\n",
    "    return x,cols\n",
    "#------------------------------------------------------------------------------\n",
    "def getHRP(cov,corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr,cov=pd.DataFrame(corr),pd.DataFrame(cov)\n",
    "    #print corr\n",
    "    #plotCorrMatrix('HRP3_corrAgain.png',corr)\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist() # recover labels\n",
    "    df0=corr.loc[sortIx,sortIx] # reorder\n",
    "    #plotCorrMatrix('HRP3_corrAgain1.png',df0,labels=df0.columns)\n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    return hrp.sort_index()\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def plotCorrMatrix(path,corr,labels=None):\n",
    "    # Heatmap of the correlation matrix\n",
    "    if labels is None:labels=[]\n",
    "    mpl.pcolor(corr)\n",
    "    mpl.colorbar()\n",
    "    mpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.xticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.savefig(path)\n",
    "    mpl.clf();mpl.close() # reset pylab\n",
    "    return\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def getCLA(cov,**kargs):\n",
    "    # Compute CLA's minimum variance portfolio\n",
    "    mean=np.arange(cov.shape[0]).reshape(-1,1) # Not used by C portf\n",
    "    lB=np.zeros(mean.shape)\n",
    "    uB=np.ones(mean.shape)\n",
    "    cla=CLA.CLA(mean,cov,lB,uB)\n",
    "    cla.solve()\n",
    "    return cla.w[-1].flatten()\n",
    "#------------------------------------------------------------------------------\n",
    "def hrpMC(numIters=1e1,nObs=520,size0=5,size1=5,mu0=0,sigma0=1e-2, \\\n",
    "    sigma1F=.25,sLength=260,rebal=22):\n",
    "    # Monte Carlo experiment on HRP\n",
    "    methods=[getIVP,getHRP,getCLA,QPsol]\n",
    "    stats,numIter={i.__name__:pd.Series() for i in methods},0\n",
    "    pointers=range(sLength,nObs,rebal)\n",
    "    print 'Numiters',numIters\n",
    "    i=0\n",
    "    name = ['IVP','HRP','CLA','QPsol']\n",
    "    workbook = xlsxwriter.Workbook('WeightsSimulated.xlsx')\n",
    "    worksheet = {}\n",
    "    for func in methods:\n",
    "        funcname = name[methods.index(func)]\n",
    "        worksheet[funcname] = workbook.add_worksheet('%s'%funcname)\n",
    "    while numIter<numIters:\n",
    "        #print numIter\n",
    "        #1) Prepare data for one experiment\n",
    "        x,cols=generateData(nObs,sLength,size0,size1,mu0,sigma0,sigma1F)\n",
    "        x=pd.DataFrame(x,columns=range(1,x.shape[1]+1))\n",
    "        #print x.describe()\n",
    "        r={i.__name__:pd.Series() for i in methods}\n",
    "        #2) Compute portfolios in-sample\n",
    "        for pointer in pointers:\n",
    "            i=i+1\n",
    "            x_=x[pointer-sLength:pointer]\n",
    "            cov_,corr_,std_=np.cov(x_,rowvar=0),x_.corr(),x_.std()\n",
    "            std_diag = np.diag(std_)\n",
    "            corr_=corr_.values\n",
    "            #plotCorrMatrix('HRP3_corr%d.png'%i,corr_)\n",
    "            #3) Compute performance out-of-sample\n",
    "            x_=x[pointer:pointer+rebal]\n",
    "            for func in methods:\n",
    "                cov_mat = cov_\n",
    "                corr_mat = corr_\n",
    "                w_=func(cov=cov_mat,corr=corr_mat) # callback  \n",
    "                # Create a workbook and add a worksheet.\n",
    "                wsheet = worksheet[name[methods.index(func)]]\n",
    "                # Some data we want to write to the worksheet.\n",
    "                # Start from the first cell. Rows and columns are zero indexed.\n",
    "                row = 0\n",
    "                col = i\n",
    "                # Iterate over the data and write it out row by row.\n",
    "                for item in (w_):\n",
    "                    wsheet.write(row, col,     item)\n",
    "                    row += 1\n",
    "                r_=pd.Series(np.dot(x_,w_))\n",
    "                r[func.__name__]=r[func.__name__].append(r_)\n",
    "        #4) Evaluate and store results\n",
    "        for func in methods:\n",
    "            r_=r[func.__name__].reset_index(drop=True)\n",
    "            #print 'function return',func,r_\n",
    "            p_=(1+r_).cumprod()\n",
    "            #print \"Function and cumlative\",func,p_\n",
    "            stats[func.__name__].loc[numIter]=p_.iloc[-1]-1 # terminal return\n",
    "        numIter+=1\n",
    "    print 'Numiters',numIters\n",
    "    #5) Report results\n",
    "    #print stats\n",
    "    stats=pd.DataFrame.from_dict(stats,orient='columns')\n",
    "    stats.to_csv('stats.csv')\n",
    "    df0,df1,df2=stats.std(),stats.var(),stats.mean()\n",
    "    result = pd.concat([df2,df0,df2/df0,df1,df1/df1['getHRP']-1],axis=1)\n",
    "    result.columns=['MEAN', 'ST. DEV.','Sharpe','VARIANCE','PERFORMANCE']\n",
    "    print result\n",
    "    return result\n",
    "#------------------------------------------------------------------------------\n",
    "if __name__=='__main__':hrpMC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FURTHER RESEARCH\n",
    "The methodology introduced in this paper is flexible, scalable and admits multiple variations of the same ideas. At stage 1 they can apply alternative definitions of linkage metrics clustering algorithms; at stage 3, they can use different functions for or alternative allocation constraints. Instead of carrying out a recursive bisection, stage 3 could also split allocations top-down using the clusters from stage 1. In a future note, we will show that it is relatively straightforward to incorporate forecasted returns and Black-Litterman-style views to this hierarchical approach. In fact, the inquisitive reader may have realized that, at its core, HRP is essentially a robust procedure to avoid matrix inversions, and the same ideas underlying HRP can be used to replace many econometric regression methods, notorious for their unstable outputs (like VAR or VECM). Exhibit 9 displays a large correlation matrix of fixed income securities before and after clustering, with over 2.1 million entries. Traditional optimization or econometric methods fail to recognize the hierarchical structure of financial Big Data, where the numerical instabilities defeat the benefits of the analysis, resulting in unreliable and detrimental outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with some mild correlations"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
